{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8493d20-4096-4191-90db-41521fd9cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f54fb8d1-7af5-4525-acf7-54702f92fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('../models/siren_broaden_instrument')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cc4d1dc-a2ad-4e07-9bad-d9515a8c05a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental data \n",
    "\n",
    "c_q_slice1 = np.array(pd.read_csv(os.path.join(path, 'Qlist_path2_woBG.csv'), header=None)).T\n",
    "c_E_slice1 = np.array(pd.read_csv(os.path.join(path, 'Elist_path2_woBG.csv'), header=None)[0]).T\n",
    "c_sqw_slice1 = np.array(pd.read_csv(os.path.join(path, 'Sqw_path2.csv'), header=None)).T\n",
    "    \n",
    "c_q_slice2 = np.array(pd.read_csv(os.path.join(path, 'Qlist_path3_woBG.csv'), header=None)).T\n",
    "c_E_slice2 = np.array(pd.read_csv(os.path.join(path, 'Elist_path3_woBG.csv'), header=None)[0]).T\n",
    "c_sqw_slice2 = np.array(pd.read_csv(os.path.join(path, 'Sqw_path3.csv'), header=None)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7fda11-a8ce-4e5a-86d1-ace4a7f44c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neutrons:  893.0\n"
     ]
    }
   ],
   "source": [
    "num_neutrons = [] \n",
    "mc_simulations = [] \n",
    "preds = [] \n",
    "\n",
    "path = '../data_experimental/'\n",
    "\n",
    "# parameters for gradient descent \n",
    "batch_size = 2048 # number of pixels across both slices \n",
    "max_iter = 2000\n",
    "learning_rate = 0.01\n",
    "n_restarts = 10 \n",
    "\n",
    "# note n_samples != n_neutrons since the rejection sampling procedure is not very efficient \n",
    "for n_samples in tqdm([5000, 10000, 25000, 50000, 75000, 100000, 115000, 250000, 350000, 500000, 750000, 1000000,1500000, 2000000,5000000, 6000000]):\n",
    "    \n",
    "    # Sample from experimentally smoothed \"probability\" distribution \n",
    "    c_sqw_rs_slice1 = rejection_sampling(c_sqw_slice1, n_samples=n_samples).T\n",
    "    c_sqw_rs_slice2 = rejection_sampling(c_sqw_slice2, n_samples=n_samples).T\n",
    "    \n",
    "    mc_simulations.append([c_sqw_rs_slice1, c_sqw_rs_slice2]) # save mc simulation \n",
    "        \n",
    "    # convert data to (q,E) from pixel space \n",
    "    test_x_1, test_y_1 = image_to_coords(c_sqw_rs_slice1, c_E_slice1, c_sqw_slice1, background_start=150,background_end=160)\n",
    "    test_x_2, test_y_2 = image_to_coords(c_sqw_rs_slice2, c_E_slice2, c_sqw_slice2, background_start=95,background_end=100)\n",
    "    \n",
    "    # convert to tensors \n",
    "    test_x = tf.convert_to_tensor(np.vstack((test_x_1, test_x_2[test_y_2 != 0])), dtype=tf.float32)\n",
    "    test_y = tf.convert_to_tensor(np.concatenate((test_y_1, test_y_2[test_y_2 != 0])), dtype=tf.float32)\n",
    "    \n",
    "    # count total number of neutrons (i.e. just sum image)\n",
    "    n_1 = np.sum(c_sqw_rs_slice1)\n",
    "    n_2 = np.sum(c_sqw_rs_slice2)\n",
    "    num_neutrons.append([n_1 + n_2])\n",
    "    \n",
    "    print('Number of neutrons: ', n_1 + n_2)\n",
    "    \n",
    "    pred_iter = [] # stores the result of n_restart gradient descent optimizations \n",
    "    \n",
    "    for n in range(n_restarts):\n",
    "        min_loss, min_loss_j1, min_loss_j2, metrics  = optimize_surrogate(test_x, test_y, \n",
    "                                                                          model, learning_rate = learning_rate,\n",
    "                                                                          batch_size = batch_size, \n",
    "                                                                          max_iter=max_iter, plotting = False)\n",
    "        pred_iter.append([min_loss_j1, min_loss_j2, min_loss])\n",
    "        \n",
    "    preds.append(pred_iter)\n",
    "    \n",
    "    print(\"number of neutrons: \", n_1 + n_2, \"Median prediction: \", np.median(pred_iter,axis=0), \"Std prediction: \", np.std(pred_iter,axis=0))\n",
    "    \n",
    "preds = np.array(preds)\n",
    "num_neutrons = np.array(num_neutrons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9866df72-3f44-43e4-981f-f55ceb03bc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
